{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e610007-3e5d-46d2-bf60-5f43372e9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import re\n",
    "import os\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from chromadb import Settings\n",
    "from typing import List, Dict\n",
    "from IPython.display import clear_output\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c2ade95-e7a9-47a1-8d66-8c8be8b47710",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./raw_database/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41658855-f009-43e0-a714-6b1c11eea575",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"./persistent_db\",\n",
    "    settings= chromadb.config.Settings(allow_reset=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53efa651-cb22-4846-93ce-fca3a85af4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"cv_doc\",\n",
    "    metadata= {\"hnsw:space\" :\"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf5a5e58-b515-466b-b3c9-cd659ca6c478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection(name=cv_doc)\n"
     ]
    }
   ],
   "source": [
    "print(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb21957a-2dd0-45bc-b87f-2374f2e5f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"Que proyectos ha desarrollado Martin?\"],\n",
    "    n_results=6,\n",
    "    include = [\"documents\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85779baa-a8c0-4357-9ddd-c00202010728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c002b83-3be9-4579-b051-4999dd9337dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_markdown_text(document_name: str, document_data: str) -> Dict:\n",
    "    # Extract the data from the differents sections of the document\n",
    "    title_match = re.search(r\"% title:\\s*(.*)\", document_data)\n",
    "    document_match = re.search(r\"% document: \\s*(.*)\", document_data)\n",
    "    document_match = re.search(r\"% document: \\s*(.*)\", document_data)\n",
    "    url_source_match = re.search(r\"% url_source:\\s*(.*)\", document_data)\n",
    "    web_source_match = re.search(r\"% web_source:\\s*(.*)\", document_data)\n",
    "    date_publication_match = re.search(r\"% date_publication:\\s*(.*)\", document_data)\n",
    "    content_match = re.search(r\"% content:\\s*(.*)\", document_data, re.DOTALL)\n",
    "    \n",
    "    #Converts the text into a json object\n",
    "    parsed_document = {\n",
    "        \"title\" : title_match.group(1).strip() if title_match else \"Unknow title document\",\n",
    "        \"document\" : document_match.group(1).strip() if document_match else \"No document data\",\n",
    "        \"document_name\" : document_name,\n",
    "        \"url_source\" : url_source_match.group(1).strip() if url_source_match else \"https://github.com/Martingago\",\n",
    "        \"web_source\" :  web_source_match.group(1).strip() if web_source_match else \"https://martingago.dev/\",\n",
    "        \"date_publication\" : date_publication_match.group(1).strip() if date_publication_match else \"No date\",\n",
    "        \"content\": content_match.group(1).strip() if content_match else \"\"\n",
    "    }\n",
    "    return parsed_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1156aa79-10b3-4531-823f-17855dc9cfb9",
   "metadata": {},
   "source": [
    "### Splits the document into small chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41b17a44-f944-441b-ace2-63792574e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_from_string(document: str) -> List[str]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 350,\n",
    "    chunk_overlap = 30,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    "    )\n",
    "    splitted_text = text_splitter.split_text(document)\n",
    "    return splitted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa76e1da-69f8-470a-becd-f4c40b221fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks_from_string(file_path: str) -> List[Dict] :\n",
    "    #Reads the doc\n",
    "    with open(file_path ,'r', encoding='utf-8') as f:\n",
    "        document_data = f.read()\n",
    "        document_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    parsed_doc = parse_markdown_text(document_name, document_data)\n",
    "    content_chunks = split_text_from_string(parsed_doc[\"content\"])\n",
    "\n",
    "    chunk_with_metadata = []\n",
    "\n",
    "    for chunk in content_chunks:\n",
    "        hash_object = hashlib.sha256(chunk.encode('utf-8')) #generates a hash in base of content to avoid duplicates\n",
    "        chunk_id = hash_object.hexdigest()\n",
    "        \n",
    "        chunk_metadata = {\n",
    "            \"document\" :  parsed_doc[\"document\"],\n",
    "            \"document_name\" : parsed_doc[\"document_name\"],\n",
    "            \"title\" : parsed_doc[\"title\"],\n",
    "            \"url_source\" : parsed_doc[\"url_source\"],\n",
    "            \"web_source\" :  parsed_doc[\"web_source\"],\n",
    "            \"date_publication\" : parsed_doc[\"date_publication\"]\n",
    "        }\n",
    "        chunk_data = {\n",
    "            \"id\": \"ID\"+chunk_id,\n",
    "            \"document\": chunk,\n",
    "            \"metadata\": chunk_metadata\n",
    "        }\n",
    "        chunk_with_metadata.append(chunk_data)\n",
    "    return chunk_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61aa03ef-3790-40f2-8004-9c0112909be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_chunks_to_collection(document_chunks, collection):\n",
    "    \n",
    "    for chunk in document_chunks:\n",
    "        # Validates if a documents exists on the database or not\n",
    "        item = collection.get(ids=[chunk[\"id\"]])\n",
    "        if not len(item[\"ids\"]) :\n",
    "            collection.add(\n",
    "                ids =[chunk[\"id\"]],\n",
    "                documents = [chunk[\"document\"]],\n",
    "                metadatas = [chunk[\"metadata\"]]\n",
    "            )\n",
    "            print(f\"    >Chunk: {chunk['id']} successfully added\")\n",
    "        else : \n",
    "            print(\"Duplicate entry, skipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadd2c1-1461-4bca-8aa6-03080ef09cc7",
   "metadata": {},
   "source": [
    "### Inserts a single file into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f75b106-2a1c-4922-8fe5-970fec3b4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_file_to_database(file_path: str) :\n",
    "    print(\"executing\")\n",
    "    document_chunks = create_chunks_from_string(file_path) # Generates an array of chunks from a file path\n",
    "    upload_chunks_to_collection(document_chunks, collection) # upload the chunks to the  specified collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db17ad47-76a6-42f4-a575-366b3ab87633",
   "metadata": {},
   "source": [
    "## Insert all files from a dir into the selected collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b03e9c34-72b9-4519-80fb-f95685a49a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_folders_upload(input_directory : str, collection) :\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "        for file in files:\n",
    "            if file.endswith('.md') and not file.startswith('.'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                time_start = datetime.now()\n",
    "                insert_file_to_database(file_path) # handle the document path and insert data into database\n",
    "                time_end = datetime.now()\n",
    "                execution_time = (time_end - time_start).total_seconds() * 1000\n",
    "                #clear_output(wait=True)\n",
    "                print(f\"[{execution_time:.0f}ms]: {file} transformed into chunks and inserted into:{collection.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bf38cdf-7b88-421c-aaee-e8e0e8d33ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing\n",
      "    >Chunk: ID7d97a620623d7a455003238a83e3e4ac0d4da933a37b63c0e98b2b39264ffee4 successfully added\n",
      "    >Chunk: ID98028029b548ce806a3a24a877556502128689308b34dcd81d8dfbdb6652930d successfully added\n",
      "    >Chunk: ID4ca74782ec1456599a5be3e9e8d0c54b3beba303111c62de85a11216afd22fd0 successfully added\n",
      "    >Chunk: IDf888865d2213a6c85c1b89ca0a40842b83ea9ef8101ca1258d6516fe18a626ee successfully added\n",
      "    >Chunk: ID018e1e14b6613f3879627cb7a2b1b8f0ede0ba211e2ac82baac066c261fcd68f successfully added\n",
      "    >Chunk: ID3771cc3f5ed1e9e255d3bb71f6b955ab6fa5d0b3538ea128a3b2ef036091f370 successfully added\n",
      "    >Chunk: ID02125f52abc0bc2a0b530394bb075095b8a9bcad009de7217b75ce13f5ba566d successfully added\n",
      "[256ms]: bravus.md transformed into chunks and inserted into:cv_doc\n",
      "executing\n",
      "    >Chunk: ID57a320c5bc9caf05199871aef8e0b4c49b6c477cfaf5f86d35ed26378b654525 successfully added\n",
      "    >Chunk: ID4ae6d97a4d4a2a545098801f2121f454ceffa9b91525fe008d76f6210a867e08 successfully added\n",
      "    >Chunk: ID8c4055abeeb31769561dec5d11d9ce2e708830805c6939c319ff747a53540529 successfully added\n",
      "    >Chunk: ID07e53344e725b3b72db1933e0e2d6f4b82e610702fa887f6417b7d24313af27f successfully added\n",
      "    >Chunk: IDa167b7e7a428722acaeff2ba0bf16bf6f06906ed771680226c5b84ca08ac2205 successfully added\n",
      "    >Chunk: ID4fee50ad71f01fd2e91f7ed98843ea3e2fa17e938c123a1f0bb803377f2ab0bc successfully added\n",
      "    >Chunk: IDb5284d23c33ad2f04f470df85fdea11b7a7520e3988fe4af8d909baf37d9b256 successfully added\n",
      "    >Chunk: IDeb9e91ee9b5315b0ee63076fa851b48ebbc098b2cb92f5e149d8e5e062e91105 successfully added\n",
      "    >Chunk: ID3b8469e8aa491e9dcbcab60922e9fe7583e4e256e55bde59c1916692f43d5c42 successfully added\n",
      "    >Chunk: IDd4c953a99c657bc7499b76d0d447e59552d98186fae48f21b002c732735085fb successfully added\n",
      "[335ms]: connect4.md transformed into chunks and inserted into:cv_doc\n",
      "executing\n",
      "    >Chunk: ID668ced6736d82bd9010d9a535ad4f4fffe8926293c4b8d37b2cdadb424eee426 successfully added\n",
      "    >Chunk: ID6c65c2fcb4d27a67d00d93426f3f102177fcf3eaacf00874ff6c50e6d2769b43 successfully added\n",
      "    >Chunk: IDfa8db828bd80546b2fa3bf278404a2cf66813a89f82bf23e9831f98e70be43b0 successfully added\n",
      "    >Chunk: ID049de927e0a55c4a00a4b389c0f3eab569776589f7a47a62c63c26cfda5971af successfully added\n",
      "    >Chunk: IDeeab3ce35146b4139e4650792bc4913cc3f027fab414a26baada34c7b14edf76 successfully added\n",
      "    >Chunk: ID6190776ef6d4233127063ac1f94d234d62095852bda7db78e90f73d4ced4ec7d successfully added\n",
      "    >Chunk: IDc6e0e3ec727cc49b3d104f9ca1bcd11c84dd37e379df59a988294b883eb46d38 successfully added\n",
      "[246ms]: password_generator.md transformed into chunks and inserted into:cv_doc\n",
      "executing\n",
      "    >Chunk: ID91e2f950b32b1f508bf221c8072f45850d47939ef048681bd89ccf9cb2e57393 successfully added\n",
      "    >Chunk: ID3a168063dc17803eb890aa207b5f2b7360b5881d8ad7dc1c76a89d07a29fa12d successfully added\n",
      "    >Chunk: IDe31327868c6d7830998794abbc702feb0d7d5371535fd8d4162e26e210c22422 successfully added\n",
      "    >Chunk: ID32c5d8e6cca72d680728a234f94f63c5c24feb99c7820af6208189e6524070fa successfully added\n",
      "    >Chunk: IDf2de4007e84f7de7d58b466fd6b98728147ebbf9eeb6040837878c61832500b7 successfully added\n",
      "[198ms]: senllap.md transformed into chunks and inserted into:cv_doc\n"
     ]
    }
   ],
   "source": [
    "handle_folders_upload(DATA_PATH, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "160f1db9-1eaf-4483-99e0-09384e1bfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chroma_client.delete_collection(\"cv_doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c927ea79-738c-4f08-9e6f-7f81dfe5325f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm_chatbot)",
   "language": "python",
   "name": "llm_chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
